{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Modoap_Lexicometrie.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "BpPcYNLHQ_uW",
        "Y9Msho6654Q3",
        "1DXLjMf3Rl-z",
        "DYTaUF776Ia-",
        "wnKuBExrSEJV",
        "cHL3awcA6UrE",
        "p9vPI_EeCTd2",
        "lwDMntCSf6Tw"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZ1LUJLXrN-B"
      },
      "source": [
        "# ModOAP - Calculs lexicométriques sur un corpus\n",
        "\n",
        "Ce script permet de calculer des informations lexicométriques d'une liste de termes au sein d'un corpus de textes numérisés.\n",
        "\n",
        "La liste de termes à rechercher doit être fournie dans la dernière cellule sous forme d'un fichier texte contenant un terme par ligne.\n",
        "\n",
        "Le corpus à interroger doit être fourni sous forme d'un dossier contenant un ou plusieurs documents Gallica au format json (issus d'un téléchargement depuis Gallica grâce au script Modoap - Téléchargement de texte structuré) (un fichier par document).\n",
        "\n",
        "Lors d'une première utilisation, le corpus doit subir un pré-traitement linguistique (création d'un fichier .nlp par fichier .json).\n",
        "\n",
        "Une fois les fichiers .nlp générés, le corpus pré-traité peut être rapidement importé.\n",
        "\n",
        "Le résultat des calculs est un fichier tableau Google Sheet créé et partagé sur le compte synchronisé, contenant les informations. \n",
        "\n",
        "Ces résultats peuvent ensuite être représentés dans une interface Web (voir :https://modoap.huma-num.fr/romans_scolaires/)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Calculs lexicométriques réalisés :**\n",
        "\n",
        "- Nombre total de mots du corpus\n",
        "- Compte des occurrences de chaque terme\n",
        "- Répartition des termes dans le corpus\n",
        "- Répartition des termes par :\n",
        "  - titre de document\n",
        "  - année de publication\n",
        "  - éditeur de document\n",
        "  - auteur de document\n",
        "- Fréquences des co-occurrences du terme dans une fenêtre donnée :\n",
        "  - pour toutes les co-occurrences\n",
        "  - pour les noms uniquement\n",
        "  - pour les verbes uniquement\n",
        "  - pour les adjectifs uniquement\n",
        "\n",
        "- Concordances des occurrences de chaque terme dans une fenêtre donnée\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "AbexKVeMoaD7"
      },
      "source": [
        "#@title ##  Préparation\n",
        "\n",
        "#@markdown ### Lancer cette cellule en préambule\n",
        "#@markdown Puis cliquer sur le lien généré pour synchroniser un compte Google Drive si demandé\n",
        "\n",
        "# chargement d'un google drive\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "if not os.path.exists(\"/content/drive/MyDrive/\") :\n",
        "  drive.mount('/content/drive/')\n",
        "\n",
        "import pickle\n",
        "!pip install -q -U spacy\n",
        "import spacy\n",
        "!python -m spacy download fr_core_news_lg\n",
        "from spacy.tokens import DocBin\n",
        "\n",
        "import fr_core_news_lg\n",
        "\n",
        "nlp = fr_core_news_lg.load() ## PIPE SPEC\n",
        "nlp = spacy.load(\"fr_core_news_lg\", disable=[\"attribute_ruler\", \"textcat_multilabel\", \"textcat\", \"ner\", \"entity_linker\", \"entity_ruler\", \"entity_linker\"])\n",
        "nlp.max_length = 1500000\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "from nltk.probability import FreqDist\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "import json\n",
        "import glob\n",
        "import pandas as pd\n",
        "import altair as alt\n",
        "from functools import partial\n",
        "from tqdm import tqdm\n",
        "tqdm = partial(tqdm, position=0, leave=True)\n",
        "\n",
        "!pip install colorama\n",
        "\n",
        "from colorama import Fore, Back, Style\n",
        "import re\n",
        "\n",
        "\n",
        "def lemmatisation(doc):\n",
        "  lemmes = [mot.lemma_ for mot in doc if mot.pos_ is not \"PUNCT\" and mot.pos_ is not \"SPACE\"]\n",
        "  return lemmes\n",
        "  \n",
        "def stopwords_filter(doc):\n",
        "  mots_grammaticaux = set(stopwords.words('french'))\n",
        "  add = [\"aussi\", \"quelque\", \"quelques\", \"tel\", \"telle\", \"telles\", \"tels\", \"y\"]\n",
        "  for mot in add :\n",
        "    mots_grammaticaux.add(mot)\n",
        "  POS_filter = [\"SPACE\", \"ADP\", \"DET\", \"CCONJ\", \"PUNCT\", \"NUM\", \"PRON\"]\n",
        "  chars = list(set('$,./?\\;\\'\\'\\’\\’\\\\^')) + [\"\\n\",\"\\xa0\"]\n",
        "  mots_lexicaux = [mot for mot in doc if len(mot.text) > 1 and mot.text.lower() not in mots_grammaticaux and not any(pos in mot.pos_ for pos in POS_filter) and not any((c in chars) for c in mot.text)]\n",
        "  return mots_lexicaux\n",
        "\n",
        "def lemma_stopwords_filter(doc):\n",
        "  mots_grammaticaux = set(stopwords.words('french'))\n",
        "  add = [\"aussi\", \"quelque\", \"quelques\", \"tel\", \"telle\", \"telles\", \"tels\"]\n",
        "  for mot in add :\n",
        "    mots_grammaticaux.add(mot)\n",
        "  POS_filter = [\"SPACE\", \"ADP\", \"DET\", \"CCONJ\", \"PUNCT\", \"NUM\", \"PRON\"]\n",
        "  chars = list(set('$,./?\\;\\'\\'\\’\\’\\\\^')) + [\"\\n\",\"\\xa0\"]\n",
        "  mots_lexicaux = [mot.lemma_ for mot in doc if len(mot.text) > 1 and mot.text.lower() not in mots_grammaticaux and not any(pos in mot.pos_ for pos in POS_filter) and not any((c in chars) for c in mot.text)]\n",
        "  return mots_lexicaux \n",
        "\n",
        "def premier_filtre(doc):\n",
        "  POS_filter = [\"SPACE\", \"PUNCT\"]\n",
        "  chars = list(set('$,./?\\;\\'\\'\\’\\’\\\\^')) + [\"\\n\",\"\\xa0\"]\n",
        "  mots_filtres = [mot for mot in doc if len(mot.text) > 1 and not any((c in chars) for c in mot.text) and not any(pos in mot.pos_ for pos in POS_filter)]\n",
        "  return mots_filtres\n",
        "\n",
        "def json2dict(jsonfile) :\n",
        "  with open(jsonfile, \"r\") as json_in :\n",
        "    dictjson = json.load(json_in)\n",
        "  return dictjson\n",
        "\n",
        "def nettoyage_bloc(bloc):\n",
        "  bloc = bloc.replace(\" fig \", \"\")\n",
        "  bloc = bloc.replace(\" do \", \" de \")\n",
        "  bloc = bloc.replace(\" dos \", \" des \")\n",
        "  bloc = bloc.replace(\" * \", \"\")\n",
        "  bloc = bloc.replace(\" k \", \"\")\n",
        "  bloc = bloc.replace(\"«\", \"\")\n",
        "  bloc = bloc.replace(\"»\", \"\")\n",
        "  bloc = bloc.replace(\" lo \", \" le \")\n",
        "  bloc = bloc.replace(\" los \", \" les \")\n",
        "  bloc = bloc.replace(\" ot \", \" et \")\n",
        "  bloc = bloc.replace(\" ôt \", \" et \")\n",
        "  bloc = bloc.replace(\"- \", \"\")\n",
        "  bloc = bloc.replace(\" - \", \"\")\n",
        "  bloc = bloc.replace(\"  \", \" \")\n",
        "  bloc = bloc.replace(\"   \", \" \")\n",
        "  bloc = bloc.replace(\" so \", \" se \")\n",
        "  bloc = bloc.replace(\" quo \", \" que \")\n",
        "  bloc = bloc.replace(\" Ello \", \" Elle \")\n",
        "  bloc = bloc.replace(\" ello \", \" elle \")\n",
        "  bloc = bloc.replace(\" villo \", \" ville \")\n",
        "  bloc = bloc.replace(\" ollo \", \" elle \")\n",
        "  bloc = bloc.replace(\" uno \", \" une \")\n",
        "  bloc = bloc.replace(\" Cotte \", \" Cette \")\n",
        "  return bloc\n",
        "\n",
        "\n",
        "def pretraitements_paragraphe(dictjson) :\n",
        "  docs = []\n",
        "  for tb in dictjson[\"Text_Blocks\"]:\n",
        "    bloc = nettoyage_bloc(dictjson[\"Text_Blocks\"][tb][\"Content\"])\n",
        "    docs.append(nlp(bloc))\n",
        "  doc_bin = DocBin(docs=docs)\n",
        "  return doc_bin\n",
        "\n",
        "def pre_traitement_dossier(chemin_dossier):\n",
        "  doc_bins = []\n",
        "  nlp_traites = [nlpfile for nlpfile in glob.glob(os.path.join(chemin_dossier,\"*.nlp\"))]\n",
        "  for jsonfile in tqdm(glob.glob(os.path.join(chemin_dossier,\"*.json\"))) :\n",
        "    if jsonfile[:-5]+\".nlp\" not in nlp_traites :\n",
        "      dictjson = json2dict(jsonfile)\n",
        "      doc_bin = pretraitements_paragraphe(dictjson)\n",
        "      doc_bin.to_disk(jsonfile[:-5]+\".nlp\")\n",
        "      doc_bins.append(pretraitements_paragraphe(dictjson))\n",
        "  return doc_bins"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhLXW1zbCMvx"
      },
      "source": [
        "# Définition du corpus\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOC7iAGRln4i",
        "cellView": "form"
      },
      "source": [
        "#@markdown # Prétraiter un dossier corpus\n",
        "\n",
        "#@markdown #### Entrer le chemin d'un dossier contenant des documents au format .json à pré-traiter :\n",
        "chemin_dossier = \"\" #@param {type:\"string\"}\n",
        "#@markdown Crée un fichier .nlp par fichier json dans le même dossier\n",
        "\n",
        "#@markdown Exemple de chemin: /content/drive/My Drive/datasets/\n",
        "\n",
        "#@markdown Le temps de calcul est long, nécessite d'être lancé une fois pour toutes.\n",
        "\n",
        "doc_bins_corpus = pre_traitement_dossier(chemin_dossier)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sgESA81gNfxN",
        "cellView": "form"
      },
      "source": [
        "#@markdown # Importer un dossier corpus prétraité:\n",
        "\n",
        "#@markdown #### Entrer le chemin d'un dossier contenant des documents déjà pré-traités, au format .nlp :\n",
        "chemin_dossier = \"\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown Exemple de chemin: /content/drive/My Drive/datasets/\n",
        "\n",
        "dico_corpus = {}\n",
        "\n",
        "jsons = [fil for fil in glob.glob(os.path.join(chemin_dossier,\"*.json\"))]\n",
        "nlpz = [fil for fil in glob.glob(os.path.join(chemin_dossier,\"*.nlp\"))]\n",
        "\n",
        "for json_file in tqdm(jsons) :\n",
        "  nlpfile = json_file[:-5]+\".nlp\"\n",
        "  dict_man = json2dict(json_file)\n",
        "  doc_bin = DocBin().from_disk(nlpfile)\n",
        "  docs = list(doc_bin.get_docs(nlp.vocab))\n",
        "  dict_man['Infos_Doc']['docs'] = docs\n",
        "  dict_man['Infos_Doc']['nlp'] = nlpfile\n",
        "  i = 0\n",
        "  for tb in dict_man['Text_Blocks'] :\n",
        "    dict_man['Text_Blocks'][tb][\"Num_Paragraphe\"] = i\n",
        "    i+= 1\n",
        "  dico_corpus[json_file] = dict_man\n",
        "\n",
        "# Transformer un JSON_CORPUS pour que l'ID des tb soit [1,2,3..]\n",
        "dico_corpus_ids = {}\n",
        "for man in tqdm(dico_corpus) :\n",
        "  i = 0\n",
        "  infodoc = dico_corpus[man][\"Infos_Doc\"]\n",
        "  tbz = {}\n",
        "  for tb in dico_corpus[man][\"Text_Blocks\"] :\n",
        "    tbz[i] = dico_corpus[man][\"Text_Blocks\"][tb]\n",
        "    i+=1\n",
        "  manew = {\"Infos_Doc\" : infodoc, \"Text_Blocks\" : tbz}\n",
        "  dico_corpus_ids[man] = manew\n",
        "\n",
        "dico_corpus = dico_corpus_ids\n",
        "\n",
        "print()\n",
        "print(\"{} documents importés\".format(len(jsons)))\n",
        "print(nlpz)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spb9VmBDTBR0"
      },
      "source": [
        "# Calculs lexicométriques"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "516WvlgE_xiF",
        "cellView": "form",
        "outputId": "986d1caf-ef0c-42ff-8e5a-6b3d665a3480"
      },
      "source": [
        "### CALCUL DES PERSONNAGES OK +++++++++++++++ AJOUT DES FREQS RELATIVES\n",
        "\n",
        "\n",
        "from spacy.matcher import Matcher\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "import gspread\n",
        "from oauth2client.client import GoogleCredentials\n",
        "gc = gspread.authorize(GoogleCredentials.get_application_default())\n",
        "\n",
        "\n",
        "#@markdown ## Calculs Lexicométriques\n",
        "#@markdown Renseigner les paramètres avant de lancer la cellule\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown #### Entrer le chemin vers un fichier texte contenant les termes à repérer :\n",
        "chemin_fichier_termes = \"/content/drive/MyDrive/100PERSOS.txt\" #@param {type:\"string\"}\n",
        "#@markdown Un fichier .txt contenant les termes à rechercher dans le corpus : un terme par ligne. \n",
        "\n",
        "#@markdown ---\n",
        "#@markdown #### Entrer le nom du tableau Google Sheet à créer :\n",
        "nom_tableau = \"Personnages\" #@param {type:\"string\"}\n",
        "#@markdown Un fichier Google Sheet portant ce nom sera créé sur le Drive\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown #### Définir la fenêtre de voisinage pour le calcul des co-occurrences et des concordances :\n",
        "fenetre_voisinage = 30 #@param {type:\"integer\"}\n",
        "#@markdown Correspond au nombre de mots dans les voisinages gauche et droit de chaque occurrence considérée. \n",
        "#@markdown Par défaut : 30 \n",
        "\n",
        "#@markdown ---\n",
        "#@markdown #### Ajouter des termes à filtrer lors du calcul, séparés par / :\n",
        "mots_a_filtrer = \"\" #@param {type:\"string\"}\n",
        "#@markdown Exemple : que/faire/il filtrera les termes \"que\", \"faire\" et \"il\" qui ne seront pas pris en compte et n'apparaîtront pas dans les résultats.\n",
        "\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "\n",
        "if not fenetre_voisinage :\n",
        "  fenetre_voisinage = 30\n",
        "\n",
        "sh = gc.create(nom_tableau) \n",
        "\n",
        "liste_stop = [\"-\", \"y\", \"1\", \",\",\",\", \"er\",\".\",\",\", \"\"]\n",
        "liste_stop = liste_stop + [x for x in mots_a_filtrer.split(\"/\")]\n",
        "\n",
        "with open(chemin_fichier_termes, \"r\") as listin : \n",
        "  listpers = listin.read()\n",
        "\n",
        "liste_persos = [x.strip() for x in listpers.split(\"\\n\")]\n",
        "freqTotales_personnages = {}\n",
        "dict_nbmots_romans = {}\n",
        "\n",
        "print()\n",
        "print(\"Calculs lexicométriques sur {} termes et création d'un fichier Google Sheet contenant les résultats : \".format(len(liste_persos)))\n",
        "\n",
        "for perso in tqdm(liste_persos) :  \n",
        "\n",
        "\n",
        "  matcher = Matcher(nlp.vocab)\n",
        "\n",
        "  if \"Darc\" in perso :\n",
        "    pattern = [{\"LOWER\": \"jeanne\"},{\"LOWER\": \"darc\"} ] \n",
        "    pattern2 = [{\"LOWER\": \"jeanne\"}, {\"LENGTH\": 2}, {\"LOWER\": \"arc\"}] \n",
        "    matcher.add(\"custom\", [pattern, pattern2])\n",
        "\n",
        "  elif \"Napoléon_Ier\" in perso :\n",
        "    pattern = [{\"LOWER\": {\"NOT_IN\" : [\"louis-\"]}}, {\"LOWER\": \"napoléon\"},{\"LOWER\": {\"NOT_IN\" : [\"iii\",\"111\"]}}]  \n",
        "    matcher.add(\"custom\", [pattern])\n",
        "\n",
        "  elif \"Napoléon_III\" in perso :\n",
        "    pattern = [{\"LOWER\": \"napoléon\"},{\"LOWER\": \"iii\"} ] \n",
        "    pattern2 = [{\"LOWER\": \"louis-\"}, {\"LOWER\": \"napoléon\"}]\n",
        "    pattern3 = [{\"LOWER\": \"napoléon\"}, {\"ORTH\": \"111\"}] \n",
        "    matcher.add(\"custom\", [pattern, pattern2, pattern3])\n",
        "\n",
        "  elif\"Quint\" in perso :\n",
        "    pattern = [{\"LEMMA\": \"Charles\"}, {\"TEXT\": \"-\"} ,{\"LEMMA\": \"Quint\"}]\n",
        "    matcher.add(\"custom\", [pattern])\n",
        "\n",
        "  elif\"Duguay\" in perso :\n",
        "    pattern = [{\"LEMMA\": \"Duguay\"}, {\"TEXT\": \"-\"} ,{\"LEMMA\": \"Trouin\"}]\n",
        "    matcher.add(\"custom\", [pattern])\n",
        "\n",
        "  elif \" \" in perso :\n",
        "    tokenlist = perso.split(\" \")\n",
        "    if len(tokenlist) == 2 :\n",
        "      pattern = [{\"TEXT\": tokenlist[0] },{\"TEXT\": tokenlist[1]} ] \n",
        "      matcher.add(\"custom\", [pattern])\n",
        "\n",
        "    elif len(tokenlist) == 3 :\n",
        "      pattern = [{\"TEXT\": tokenlist[0] },{\"TEXT\": tokenlist[1]},{\"TEXT\": tokenlist[2]} ] \n",
        "      matcher.add(\"custom\", [pattern])\n",
        "\n",
        "    else : print(\"Problème avec le nom du personnage\")\n",
        "\n",
        "  else :\n",
        "    pattern = [{\"TEXT\": perso}] \n",
        "    matcher.add(\"custom\", [pattern])\n",
        "\n",
        "  cooc_global = []\n",
        "  cooc_adj = []\n",
        "  cooc_verb = []\n",
        "  cooc_nom = []\n",
        "  resultats_annees = {}\n",
        "  resultats_romans = {}\n",
        "  resultats_editeur = {}\n",
        "  resultats_auteur = {}\n",
        "  concordances = [] \n",
        "\n",
        "  # Itération sur les romans du corpus\n",
        "  patrons = []\n",
        "  compteur_matches = 0\n",
        "  for man in dico_corpus :\n",
        "\n",
        "    nb_mots_roman = 0\n",
        "    annee = dico_corpus[man][\"Infos_Doc\"][\"Publication_Date\"]\n",
        "    roman = dico_corpus[man][\"Infos_Doc\"][\"Titre\"]\n",
        "    editeur = dico_corpus[man][\"Infos_Doc\"][\"Editeur\"]\n",
        "    auteur = dico_corpus[man][\"Infos_Doc\"][\"Auteur\"]\n",
        "    \n",
        "    docs = dico_corpus[man][\"Infos_Doc\"][\"docs\"]\n",
        "    compteur_paragraphes = 0\n",
        "\n",
        "    # Itération sur les blocs texte d'un roman\n",
        "    for doc in docs :\n",
        "      nb_mots_paragraphe = len([w.text for w in doc if w.pos_ != \"PUNCT\"])\n",
        "      nb_mots_roman = nb_mots_roman + nb_mots_paragraphe\n",
        "    \n",
        "      matches = matcher(doc)\n",
        "      \n",
        "      # Itération sur les patrons repérés d'un bloc texte\n",
        "      for match_id, start, end in matches:\n",
        "        string_id = nlp.vocab.strings[match_id]  \n",
        "        span = doc[start:end]\n",
        "        patrons.append(span.text)\n",
        "\n",
        "        # Score Freqs totales\n",
        "        if perso in freqTotales_personnages : \n",
        "          freqTotales_personnages[perso] += 1\n",
        "        else :\n",
        "          freqTotales_personnages[perso] = 1\n",
        "\n",
        "      # Récupération de la concordance\n",
        "        #Contenu = dico_corpus[man][\"Text_Blocks\"][compteur_paragraphes][\"Content\"]\n",
        "        Roman\t= roman\n",
        "        Page_Num = dico_corpus[man][\"Text_Blocks\"][compteur_paragraphes][\"Page_Num\"]\n",
        "        if \"Napoléon_Ier\" in perso :\n",
        "          Concordance = str(doc[start-(fenetre_voisinage+1) : start+1]) + \" <mark>\"+str(doc[start+1:end-1])+\"</mark> \"+ str(doc[end-1:end+1+fenetre_voisinage])\n",
        "        else :  \n",
        "          Concordance = str(doc[start-fenetre_voisinage : start]) + \" <mark>\"+str(span)+\"</mark> \"+ str(doc[end:end+fenetre_voisinage])\n",
        "        Region = dico_corpus[man][\"Text_Blocks\"][compteur_paragraphes][\"Position\"]\n",
        "        Url_iiif_region = \"https://gallica.bnf.fr/iiif/ark:/12148/{}/f{}/{},{},{},{}/full/0/native.jpg\".format(man.split(\"_\")[-3], Page_Num, Region[\"hpos\"], Region[\"vpos\"], Region[\"width\"], Region[\"height\"])\n",
        "        Lien_iiif_region = \"<a href=\\\"\"+Url_iiif_region+\"\\\">Voir le paragraphe</a>\"\n",
        "        Url_gallica_Page = \"https://gallica.bnf.fr/ark:/12148/{}/f{}.item.texteImage\".format(man.split(\"_\")[-3], Page_Num)\n",
        "        Lien_gallica_Page = \"<a href=\\\"\"+Url_gallica_Page+\"\\\">Voir la page</a>\"\n",
        "\n",
        "        dict_concord = {\"Roman\" : roman,\n",
        "                        \"Page_Num\": Page_Num,\n",
        "                        \"Concordance\" : Concordance,\n",
        "                        \"Url_iiif_region\" : Lien_iiif_region,\n",
        "                        \"Url_gallica_Page\" : Lien_gallica_Page}\n",
        "        concordances.append(dict_concord)\n",
        "\n",
        "      # Comptes par année auteur éditeur et roman :\n",
        "        compteur_matches+=1\n",
        "        if annee in resultats_annees :\n",
        "          resultats_annees[annee] += 1\n",
        "        else :\n",
        "          resultats_annees[annee] = 1\n",
        "\n",
        "        if roman in resultats_romans :\n",
        "          resultats_romans[roman] += 1\n",
        "        else :\n",
        "          resultats_romans[roman] = 1\n",
        "          \n",
        "        if auteur in resultats_auteur :\n",
        "          resultats_auteur[auteur] += 1\n",
        "        else :  \n",
        "          resultats_auteur[auteur] = 1\n",
        "        if editeur in resultats_editeur :\n",
        "          resultats_editeur[editeur] += 1\n",
        "        else : \n",
        "          resultats_editeur[editeur] = 1\n",
        "\n",
        "      # Comptes des Cooccurrences\n",
        "        if cooc_global : \n",
        "          cooc_global = cooc_global + [str(w.lemma_) for w in doc[start-fenetre_voisinage:start]if w.is_stop == False and w.pos_ is not \"PUNCT\" and w.lemma_ not in liste_stop] + [str(w.lemma_) for w in doc[end:end+fenetre_voisinage]if w.is_stop == False and w.pos_ is not \"PUNCT\" and w.lemma_ not in liste_stop]\n",
        "        else :\n",
        "          cooc_global = [str(w.lemma_) for w in doc[start-fenetre_voisinage:start]if w.is_stop == False and w.pos_ is not \"PUNCT\" and w.lemma_ not in liste_stop] + [str(w.lemma_) for w in doc[end:end+fenetre_voisinage]if w.is_stop == False and w.pos_ is not \"PUNCT\" and w.lemma_ not in liste_stop]\n",
        "\n",
        "        if cooc_adj :\n",
        "          cooc_adj = cooc_adj + [str(w.lemma_) for w in doc[start-fenetre_voisinage:start]if w.is_stop == False and w.pos_ == \"ADJ\" and w.pos_ is not \"PUNCT\" and w.lemma_ not in liste_stop] + [str(w.lemma_) for w in doc[end:end+fenetre_voisinage]if w.is_stop == False and w.pos_ == \"ADJ\" and w.pos_ is not \"PUNCT\" and w.lemma_ not in liste_stop]\n",
        "        else :\n",
        "          cooc_adj = [str(w.lemma_) for w in doc[start-fenetre_voisinage:start]if w.is_stop == False and w.pos_ == \"ADJ\" and w.pos_ is not \"PUNCT\" and w.lemma_ not in liste_stop] + [str(w.lemma_) for w in doc[end:end+fenetre_voisinage]if w.is_stop == False and w.pos_ == \"ADJ\" and w.pos_ is not \"PUNCT\" and w.lemma_ not in liste_stop]\n",
        "\n",
        "        if cooc_verb :\n",
        "          cooc_verb = cooc_verb + [str(w.lemma_) for w in doc[start-fenetre_voisinage:start]if w.is_stop == False and w.pos_ == \"VERB\" and w.pos_ is not \"PUNCT\" and w.lemma_ not in liste_stop] + [str(w.lemma_) for w in doc[end:end+fenetre_voisinage]if w.is_stop == False and w.pos_ == \"VERB\" and w.pos_ is not \"PUNCT\" and w.lemma_ not in liste_stop]\n",
        "        else :\n",
        "          cooc_verb = [str(w.lemma_) for w in doc[start-fenetre_voisinage:start]if w.is_stop == False and w.pos_ == \"VERB\" and w.pos_ is not \"PUNCT\" and w.lemma_ not in liste_stop] + [str(w.lemma_) for w in doc[end:end+fenetre_voisinage]if w.is_stop == False and w.pos_ == \"VERB\" and w.pos_ is not \"PUNCT\" and w.lemma_ not in liste_stop]\n",
        "\n",
        "        if cooc_nom :\n",
        "          cooc_nom = cooc_nom + [str(w.lemma_) for w in doc[start-fenetre_voisinage:start]if w.is_stop == False and w.pos_ == \"NOUN\" and w.pos_ is not \"PUNCT\" and w.lemma_ not in liste_stop] + [str(w.lemma_) for w in doc[end:end+fenetre_voisinage]if w.is_stop == False and w.pos_ == \"NOUN\" and w.pos_ is not \"PUNCT\" and w.lemma_ not in liste_stop]\n",
        "        else :\n",
        "          cooc_nom = [str(w.lemma_) for w in doc[start-fenetre_voisinage:start]if w.is_stop == False and w.pos_ == \"NOUN\" and w.pos_ is not \"PUNCT\" and w.lemma_ not in liste_stop] + [str(w.lemma_) for w in doc[end:end+fenetre_voisinage]if w.is_stop == False and w.pos_ == \"NOUN\" and w.pos_ is not \"PUNCT\" and w.lemma_ not in liste_stop]\n",
        "\n",
        "      compteur_paragraphes += 1\n",
        "    if roman not in dict_nbmots_romans.keys():\n",
        "      dict_nbmots_romans[roman] = nb_mots_roman\n",
        "  nb_mots_corpus = sum(dict_nbmots_romans.values())\n",
        "  # Création du tableau de concordances :\n",
        "\n",
        "  dataviz_conco = [[k for k, v in concordances[0].items()]]\n",
        "  for dict_concord in concordances :\n",
        "    row = [v for k, v in dict_concord.items()]\n",
        "    dataviz_conco.append(row)\n",
        "\n",
        "  # Création des tableaux de fréquences par année, auteur, editeur et roman :\n",
        "\n",
        "  dataviz_freq_annees = [[\"annee\", \"frequence\"]]\n",
        "  for annee, freq in resultats_annees.items() :\n",
        "    liste = [annee, freq]\n",
        "    dataviz_freq_annees.append(liste)\n",
        "\n",
        "  dataviz_freq_roman = [[\"roman\", \"frequence\"]]\n",
        "  for roman, freq in resultats_romans.items() :\n",
        "    liste = [roman, freq]\n",
        "    dataviz_freq_roman.append(liste)\n",
        "\n",
        "  dataviz_freq_editeur = [[\"editeur\", \"frequence\"]]\n",
        "  for editeur, freq in resultats_editeur.items() :\n",
        "    liste = [editeur, freq]\n",
        "    dataviz_freq_editeur.append(liste)\n",
        "\n",
        "  dataviz_freq_auteur = [[\"auteur\", \"frequence\"]]\n",
        "  for auteur, freq in resultats_auteur.items() :\n",
        "    liste = [auteur, freq]\n",
        "    dataviz_freq_auteur.append(liste)\n",
        "\n",
        "  # Création des tableaux des fréquences des cooccurrences \n",
        "\n",
        "  fdistcooc_global = FreqDist(cooc_global)\n",
        "  fdistcooc_globalMC = fdistcooc_global.most_common(10)\n",
        "  dataviz_cooc_global = [[\"terme\", \"freq_absolue\",\"freq_relative\"]]\n",
        "  for e in fdistcooc_globalMC :\n",
        "    liste = [e[0],e[1],e[1]/nb_mots_corpus*100]\n",
        "    dataviz_cooc_global.append(liste)\n",
        "\n",
        "\n",
        "  #################################\n",
        "\n",
        "  fdistcooc_adj = FreqDist(cooc_adj)\n",
        "  fdistcooc_adjMC = fdistcooc_adj.most_common(10)\n",
        "  dataviz_cooc_adj = [[\"terme\", \"freq_absolue\",\"freq_relative\"]]\n",
        "  for e in fdistcooc_adjMC :\n",
        "    liste = [e[0],e[1],e[1]/nb_mots_corpus*100]\n",
        "    dataviz_cooc_adj.append(liste)\n",
        "\n",
        "  #################################\n",
        "\n",
        "  fdistcooc_verb = FreqDist(cooc_verb)\n",
        "  fdistcooc_verbMC = fdistcooc_verb.most_common(10)\n",
        "  dataviz_cooc_verb = [[\"terme\", \"freq_absolue\",\"freq_relative\"]]\n",
        "  for e in fdistcooc_verbMC :\n",
        "    liste = [e[0],e[1],e[1]/nb_mots_corpus*100]\n",
        "    dataviz_cooc_verb.append(liste)\n",
        "\n",
        "  #################################\n",
        "\n",
        "  fdistcooc_nom = FreqDist(cooc_nom)\n",
        "  fdistcooc_nomMC = fdistcooc_nom.most_common(10)\n",
        "  dataviz_cooc_nom = [[\"terme\", \"freq_absolue\",\"freq_relative\"]]\n",
        "  for e in fdistcooc_nomMC :\n",
        "    liste = [e[0],e[1],e[1]/nb_mots_corpus*100]\n",
        "    dataviz_cooc_nom.append(liste)\n",
        "\n",
        "                # FILL THE GSHEET\n",
        "\n",
        "  worksheet_new = sh.add_worksheet(title=perso, rows=\"100\", cols=\"100\")\n",
        "\n",
        "  sh.values_update(perso+\"!A:B\", params={'valueInputOption': 'RAW'}, body={'values': dataviz_freq_roman})\n",
        "  sh.values_update(perso+\"!C:D\", params={'valueInputOption': 'RAW'}, body={'values': dataviz_freq_annees})\n",
        "  sh.values_update(perso+\"!E:F\", params={'valueInputOption': 'RAW'}, body={'values': dataviz_freq_auteur})\n",
        "  sh.values_update(perso+\"!G:H\", params={'valueInputOption': 'RAW'}, body={'values': dataviz_freq_editeur})\n",
        "\n",
        "  sh.values_update(perso+\"!I:K\", params={'valueInputOption': 'RAW'}, body={'values': dataviz_cooc_global})\n",
        "  sh.values_update(perso+\"!L:N\", params={'valueInputOption': 'RAW'}, body={'values': dataviz_cooc_adj})\n",
        "  sh.values_update(perso+\"!O:Q\", params={'valueInputOption': 'RAW'}, body={'values': dataviz_cooc_nom})\n",
        "  sh.values_update(perso+\"!R:T\", params={'valueInputOption': 'RAW'}, body={'values': dataviz_cooc_verb})\n",
        "\n",
        "  sh.values_update(perso+\"!U:Y\", params={'valueInputOption': 'RAW'}, body={'values': dataviz_conco})\n",
        "\n",
        "\n",
        "# Tableau des fréquences totales\n",
        "\n",
        "dataviz_freqTotales_lien = [[\"Terme\", \"Frequence\"]]\n",
        "dataviz_freqTotales = [[\"Terme\"]]\n",
        "for terme, freq in freqTotales_personnages.items() :\n",
        "  lien = \"<a href=\\\"personnage.php?personnage=\"+terme+\"\\\">\"+terme+\"</a>\"  ####### LIEN DANS TABLEAU CONCORDANCES\n",
        "  liste_lien = [lien, freq]\n",
        "  liste_terme = [terme]\n",
        "  dataviz_freqTotales.append(liste_terme)\n",
        "  dataviz_freqTotales_lien.append(liste_lien)\n",
        "sh.values_update(\"Feuille 1!A:A\", params={'valueInputOption': 'RAW'}, body={'values': dataviz_freqTotales})\n",
        "sh.values_update(\"Feuille 1!B:C\", params={'valueInputOption': 'RAW'}, body={'values': dataviz_freqTotales_lien})\n",
        "\n",
        "# Tableau nombre de mots par roman\n",
        "dataviz_nbmotsRoman = [[\"Roman\", \"Nombre_de_mots\"]]\n",
        "for roman, nb_mots in dict_nbmots_romans.items():\n",
        "  liste = [roman, nb_mots]\n",
        "  dataviz_nbmotsRoman.append(liste)\n",
        "sh.values_update(\"Feuille 1!D:E\", params={'valueInputOption': 'RAW'}, body={'values': dataviz_nbmotsRoman})\n",
        "\n",
        "# Nombre de mots du corpus \n",
        "\n",
        "dataviz_nb_mots_corpus = [[\"Nombre_mots_corpus\"],[nb_mots_corpus]]\n",
        "sh.values_update(\"Feuille 1!F:F\", params={'valueInputOption': 'RAW'}, body={'values': dataviz_nb_mots_corpus})\n",
        "\n",
        "\n",
        "sh.share(\"None\", perm_type='anyone', role='reader')\n",
        "print(\"ID du spreadsheet créé et partagé : \", sh.id)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/101 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Calculs lexicométriques sur 101 termes et création d'un fichier Google Sheet contenant les résultats : \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  3%|▎         | 3/101 [00:26<14:18,  8.76s/it]"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}